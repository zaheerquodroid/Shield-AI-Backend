pattern_id: csec_36_missing_code_analysis
name: "Missing Static Code Analysis for AI-Generated Scripts"
description: |
  Detects code that executes AI-generated Python scripts without static analysis.
  AI-generated code may contain dangerous patterns (arbitrary code execution,
  file system access, network calls) that should be validated before execution.

severity: high
category: ai_security
language: python
frameworks:
  - django
  - flask
  - fastapi

cwe:
  - CWE-94   # Improper Control of Generation of Code (Code Injection)
  - CWE-95   # Improper Neutralization of Directives in Dynamically Evaluated Code
  - CWE-502  # Deserialization of Untrusted Data
  - CWE-78   # OS Command Injection

owasp:
  - A03:2021  # Injection
  - A08:2021  # Software and Data Integrity Failures

tags:
  - ai-security
  - llm-security
  - code-injection
  - static-analysis
  - code-execution
  - prompt-injection

file_patterns:
  - "**/*.py"
  - "**/script_validator.py"
  - "**/code_analyzer.py"
  - "**/ai_*.py"
  - "**/llm_*.py"

detection:
  python:
    # Detection 1: Direct use of exec() without validation
    - pattern: '\bexec\s*\('
      description: "exec() call without static analysis (dangerous)"
      severity: critical

    # Detection 2: Direct use of eval() without validation
    - pattern: '\beval\s*\('
      description: "eval() call without static analysis (dangerous)"
      severity: critical

    # Detection 3: compile() without validation
    - pattern: '\bcompile\s*\('
      description: "compile() call without validation (may need analysis)"
      severity: high

    # Detection 4: __import__() without validation
    - pattern: '\b__import__\s*\('
      description: "__import__() call without validation (dangerous)"
      severity: high

    # Detection 5: subprocess calls without validation
    - pattern: 'subprocess\.(run|call|Popen|check_output)\s*\('
      description: "subprocess call without validation (command injection risk)"
      severity: high

    # Detection 6: AI/LLM script execution pattern
    - pattern: 'def\s+execute_\w*script\s*\(.*?\):'
      description: "Script execution function (check for code analysis)"
      severity: medium

    # Detection 7: Code from LLM/AI without analysis
    - pattern: '(llm|ai|gpt|claude).*\.(generate|complete|run).*exec\('
      description: "AI-generated code executed directly (needs analysis)"
      severity: critical

metadata:
  impact: |
    Without static code analysis of AI-generated scripts:
    - Malicious code can be executed
    - Prompt injection can run arbitrary commands
    - File system can be accessed/modified
    - Network calls can exfiltrate data
    - System commands can be executed
    - Secrets/credentials can be stolen

  business_risk: |
    - Data breach via code execution
    - Server compromise
    - Lateral movement in infrastructure
    - Compliance violations (SOC 2, ISO 27001)
    - Reputation damage
    - Legal liability for AI misuse

  compliance:
    - "OWASP Top 10 for LLM Applications: LLM08 - Excessive Agency"
    - "OWASP Top 10 for LLM Applications: LLM01 - Prompt Injection"
    - "OWASP Top 10 2021: A03 - Injection"
    - "NIST AI Risk Management Framework"
    - "ISO/IEC 42001 - AI Management System"

jira_reference:
  ticket: CSEC-36
  epic: CSEC-9
  project: Coco TestAI Security Remediation

fix_strategy:
  approach: static_code_analysis
  breaking_change: false
  description: |
    Implement AST-based static code analysis for AI-generated scripts:

    1. Parse code with Python ast module
    2. Check for dangerous imports (os, subprocess, socket, etc.)
    3. Check for dangerous built-ins (exec, eval, compile, __import__)
    4. Check file operations (open, read, write, delete)
    5. Check network operations (socket, requests, urllib)
    6. Use configurable allowlist/blocklist
    7. Reject scripts that fail validation

    This is a non-breaking addition that adds safety layer.

  phases:
    - name: analyzer_creation
      duration_days: 1
      description: "Create code_analyzer.py with AST analysis"

    - name: configuration
      duration_days: 0
      description: "Add allowlist/blocklist to settings"

    - name: integration
      duration_days: 1
      description: "Integrate analyzer into script execution flow"

    - name: testing
      duration_days: 1
      description: "Test with malicious and benign scripts"

remediation:
  automated: false  # Requires integration into existing code
  manual_steps: |
    1. Create code_analyzer.py with analyze_script() function
    2. Add configurable allowlist/blocklist to settings.py
    3. Integrate analyzer before exec()/eval() calls
    4. Add comprehensive tests for malicious patterns
    5. Configure alerts for flagged scripts
    6. Document safe coding patterns for AI prompts

  fix_template: csec_36_python.py

  testing_approach: |
    1. Test with benign scripts (should pass)
    2. Test with malicious imports (should block)
    3. Test with exec/eval (should block)
    4. Test with file operations (should block if outside workdir)
    5. Test with network calls (should block)
    6. Test with allowlisted imports (should pass)
    7. Test performance overhead (<10ms per script)

examples:
  vulnerable_code: |
    # VULNERABLE: Direct execution without analysis
    def execute_ai_script(script_code):
        # LLM generates script_code
        exec(script_code)  # DANGEROUS!

    # VULNERABLE: No validation
    def run_llm_code(code):
        compiled = compile(code, '<string>', 'exec')
        exec(compiled)  # No checks!

  secure_code: |
    # SECURE: Static analysis before execution
    from code_analyzer import analyze_script

    def execute_ai_script(script_code):
        # Analyze before execution
        analysis = analyze_script(script_code)

        if not analysis['is_safe']:
            raise SecurityError(
                f"Dangerous patterns detected: {analysis['violations']}"
            )

        # Safe to execute
        exec(script_code)

    # Example violations
    analysis = analyze_script('''
    import os
    os.system('rm -rf /')  # Malicious!
    ''')
    # Result: {'is_safe': False, 'violations': ['dangerous_import: os', 'system_call']}

references:
  - https://owasp.org/www-project-top-10-for-large-language-model-applications/
  - https://python.readthedocs.io/en/stable/library/ast.html
  - https://docs.python.org/3/library/functions.html#exec
  - https://cheatsheetseries.owasp.org/cheatsheets/Injection_Prevention_Cheat_Sheet.html

notes: |
  IMPORTANT CONSIDERATIONS:

  1. AST Analysis: Parse code to detect patterns, don't execute
  2. Allowlist: Permit safe imports (math, json, datetime)
  3. Blocklist: Block dangerous imports (os, subprocess, socket)
  4. File Access: Restrict to working directory only
  5. Network: Block all network calls by default
  6. Dynamic Imports: Block __import__, importlib

  DANGEROUS PATTERNS TO DETECT:
  - import os, sys, subprocess, socket, urllib, requests
  - exec(), eval(), compile()
  - open() with 'w' or 'a' mode (write/append)
  - __import__(), importlib.import_module()
  - Attribute access to __builtins__, __globals__, __locals__

  SAFE PATTERNS (ALLOWLIST):
  - import math, json, datetime, re, typing
  - Pure data transformations
  - Calculations without side effects
  - String operations

  DEPLOYMENT CHECKLIST:
  - [ ] Code analyzer implemented with AST
  - [ ] Allowlist/blocklist configured
  - [ ] Integrated into script execution flow
  - [ ] Tests cover all dangerous patterns
  - [ ] Alerts configured for violations
  - [ ] Documentation for safe AI prompts
  - [ ] Performance tested (<10ms overhead)
